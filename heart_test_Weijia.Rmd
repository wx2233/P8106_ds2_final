---
title: "data_test_Weijia"
author: "Weijia Xiong"
date: "5/15/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,message = FALSE,warning = FALSE)
library(tidyverse)
library(caret)
library(rpart.plot)
library(MASS)
library(pROC)
library(tidyverse)
library(AppliedPredictiveModeling)
library(DataExplorer)
library(ggthemes)
library(GGally)
library(kableExtra)
library(patchwork)
```


# Load data

```{r}
# import data
heart = read_csv("heart.csv") 

data = heart %>% 
  mutate(sex = if_else(sex == 1, "male", "female"),
         fbs = if_else(fbs == 1, ">120", "<=120"),
         exang = if_else(exang == 1, "yes" ,"no"),
         target = if_else(target == 1, "pos", "neg"),
         cp = case_when(
           cp == 3 ~ "typical angina",
           cp == 1 ~ "atypical angina",
           cp == 2 ~ "non-anginal",
           cp == 0 ~ "asymptomatic angina"
                            ),
         restecg = case_when(
           restecg == 0 ~ "hypertrophy",
           restecg == 1 ~ "normal",
           restecg == 2 ~ "wave abnormality"
                                  ),
        slope = case_when(
          slope == 2 ~ "upsloping",
          slope == 1 ~ "flat",
          slope == 0 ~ "downsloping"
        ),
        thal = case_when(
          thal == 1 ~ "fixed defect",
          thal == 2 ~ "normal",
          thal == 3 ~ "reversable defect"
        ),
         cp = as.factor(cp),
         restecg = as.factor(restecg),
         slope = as.factor(slope),
         ca = as.factor(ca),
         thal = as.factor(thal)
         ) %>% 
  mutate_if(is.character, as.factor) %>% 
  dplyr::select(target, sex, fbs, exang, cp, restecg, slope, ca, thal, everything()) %>% 
  na.omit()
```

# Exploratory analysis/visualization

```{r}

theme1 <- transparentTheme(trans = .4)
theme1$strip.background$col <- rgb(.0, .6, .2, .2) 
trellis.par.set(theme1)

featurePlot(x = data[, 10:14],
            y = data$target,
            scales = list(x=list(relation="free"),
                          y=list(relation="free")), 
            plot = "density", pch = "|",
            auto.key = list(columns = 2))

```


```{r}
#ggpairs(data[,1:9])
plot_bar(data, ggtheme = theme_classic())
```

```{r}
a1 = ggplot(data) + geom_bar(aes(x = sex)) + facet_grid(. ~ target) + ggtitle("sex")
a2 = ggplot(data) + geom_bar(aes(x = fbs)) + facet_grid(. ~ target) + ggtitle("fbs")
a3 = ggplot(data) + geom_bar(aes(x = exang)) + facet_grid(. ~ target) + ggtitle("exang")
a4 = ggplot(data) + geom_bar(aes(x = cp)) + facet_grid(. ~ target) + ggtitle("cp")
a5 = ggplot(data) + geom_bar(aes(x = restecg)) + facet_grid(. ~ target) + ggtitle("restecg")
a6 = ggplot(data) + geom_bar(aes(x = slope)) + facet_grid(. ~ target) + ggtitle("slope")
a7 = ggplot(data) + geom_bar(aes(x = ca)) + facet_grid(. ~ target) + ggtitle("ca")
a8 = ggplot(data) + geom_bar(aes(x = thal)) + facet_grid(. ~ target) + ggtitle("thal")

all = (a1+a2+a3+a4)/(a5+a6+a7+a8)
all
```


# Models

```{r}
# train test set partition
set.seed(123)
rowTrain <- createDataPartition(y = data$target,
                                p = 0.75,
                                list = FALSE)
```

```{r classification tree, fig.width=7, fig.height=4}
set.seed(123)
ctrl <- trainControl(method = "repeatedcv", summaryFunction = twoClassSummary, classProbs = TRUE)
```

# GLM

```{r}
glmnGrid = expand.grid(.alpha = seq(0,1,length = 6),
                       .lambda = exp(seq(-8,-2,length = 20)))
set.seed(123)

glmn.fit.c <- train(target~., data=data[rowTrain,], 
                     method = "glmnet",
                     tuneGrid = glmnGrid,
                     metric = "ROC",
                     trControl = ctrl)

plot(glmn.fit.c, xTrans = function(x) log(x))

glmn.fit.c$bestTune

glmn.pred = predict(glmn.fit.c, newdata = data[-rowTrain,],type = "prob")[,2]

test.pred = rep("pos", length(glmn.pred))
test.pred[glmn.pred<0.5] = "neg"

confusionMatrix(data = as.factor(test.pred),
                reference = data$target[-rowTrain],
                positive = "pos")


vip_ridge = vip::vip(glmn.fit.c, num_features = 20, method = "model") + 
  ggtitle("GLM")
```

# KNN

```{r}
set.seed(123)

knn.fit.c <- train(target~., data=data[rowTrain,], 
                   method = "knn",
                   preProcess = c("center", "scale"),
                   tuneGrid = data.frame(k = seq(1,150,by = 5)),
                   trControl = ctrl)

plot(knn.fit.c)
knn.fit.c$bestTune

knn.pred = predict(knn.fit.c, newdata = data[-rowTrain,],type = "prob")[,2]

test.pred = rep("pos", length(knn.pred))
test.pred[knn.pred<0.5] = "neg"

confusionMatrix(data = as.factor(test.pred),
                reference = data$target[-rowTrain],
                positive = "pos")
```

## Tree (tune over cp)

```{r}
set.seed(123)
rpart.fit.c <- train(target~., data=data[rowTrain,], 
                     method = "rpart",
                     tuneGrid = data.frame(cp = exp(seq(-10, -5, len = 100))), 
                     trControl = ctrl,
                     metric = "ROC")

ggplot(rpart.fit.c, highlight = TRUE)
rpart.plot(rpart.fit.c$finalModel)

tree.pred = predict(rpart.fit.c, newdata = data[-rowTrain,],type = "prob")[,1]
```


```{r}
# error rate
tree_error_rate = mean(data[-rowTrain,]$target != predict(rpart.fit.c, newdata = data[-rowTrain,], type = "raw"))
```

## Random Forest
```{r random forests}
rf.grid <- expand.grid(mtry = 1:6, 
                       splitrule = "gini",
                       min.node.size = 1:6)
set.seed(123)
rf.fit.c <- train(target~., data=data[rowTrain,],
                method = "ranger",
                tuneGrid = rf.grid,
                metric = "ROC",
                trControl = ctrl,
                importance = "impurity")
ggplot(rf.fit.c, highlight = TRUE)
rf.fit.c$bestTune

rf.pred = predict(rf.fit.c, newdata = data[-rowTrain,],type = "prob")[,1]
```

```{r}
# variable importance
barplot(sort(ranger::importance(rf.fit.c$finalModel), decreasing = FALSE), 
        las = 2, horiz = TRUE, cex.names = 0.7,
        col = colorRampPalette(colors = c("cyan","blue"))(8))
```

```{r}
# error rate
rf_error_rate = mean(data[-rowTrain,]$target != predict(rf.fit.c, newdata = data[-rowTrain,], type = "raw"))
```

## Boosting
```{r boosting}
gbmB.grid <- expand.grid(n.trees = c(2000,3000,4000), 
                         interaction.depth = 1:6,
                         shrinkage = c(0.001,0.003,0.005), 
                         n.minobsinnode = 1)
set.seed(123)
gbmB.fit <- train(target~., data=data[rowTrain,],
                 tuneGrid = gbmB.grid,
                 trControl = ctrl,
                 method = "gbm",
                 distribution = "bernoulli",
                 metric = "ROC",
                 verbose = FALSE) 
ggplot(gbmB.fit, highlight = TRUE)
gbmB.fit$bestTune

gbmB.pred = predict(gbmB.fit, newdata = data[-rowTrain,],type = "prob")[,1]

```

```{r}
gbmB_pred_raw = predict(gbmB.fit, newdata = data[-rowTrain,],type = "raw")

gbmBmatrix = confusionMatrix(data = gbmB_pred_raw,
                reference = data$target[-rowTrain],
                positive = "pos")

gbmBmatrix$overall[1:2]
gbmBmatrix$byClass[1:2]
```

```{r}
# variable importance
summary(gbmB.fit$finalModel, las = 2, cBars = 19, cex.names = 0.6)
```

```{r}
# error rate
boost_error_rate = mean(data[-rowTrain,]$target != predict(gbmB.fit, newdata = data[-rowTrain,], type = "raw"))
```


## Support vector machine

### Linear kernel 

```{r}
set.seed(123)
svml.fit <- train(target~.,
                  data = data[rowTrain,],
                  method = "svmLinear2",
                  preProcess = c("center", "scale"),
                  tuneGrid = data.frame(cost = exp(seq(-5,1,len=20))), 
                  trControl = ctrl)
ggplot(svml.fit, highlight = TRUE)
svml.fit$bestTune

svml.pred = predict(svml.fit, newdata = data[-rowTrain,],type = "prob")[,1]
```

### Radial kernel

```{r}
svmr.grid <- expand.grid(C = exp(seq(-1,4,len=10)),
                         sigma = exp(seq(-6,-2,len=10)))
set.seed(123)
svmr.fit <- train(target~., 
                  data = data,
                  subset = rowTrain,
                  method = "svmRadial",
                  preProcess = c("center", "scale"), tuneGrid = svmr.grid,
                  trControl = ctrl)
ggplot(svmr.fit, highlight = TRUE) 
svmr.fit$bestTune
svmr.pred = predict(svmr.fit, newdata = data[-rowTrain,],type = "prob")[,1]
```

# ROC curve

```{r}
glm.roc = roc(data$target[-rowTrain], glmn.pred)
knn.roc = roc(data$target[-rowTrain], knn.pred)
tree.roc = roc(data$target[-rowTrain], tree.pred)
gbmB.roc = roc(data$target[-rowTrain], gbmB.pred)
rf.roc = roc(data$target[-rowTrain], rf.pred)
svml.roc = roc(data$target[-rowTrain], svml.pred)
svmr.roc = roc(data$target[-rowTrain], svmr.pred)

plot(glm.roc)
plot(knn.roc, add = TRUE, col = 2)
plot(tree.roc, add = TRUE, col = 3)
plot(gbmB.roc, add = TRUE, col = 4)
plot(rf.roc, add = TRUE, col = 5)
plot(svml.roc, add = TRUE, col = 6)
plot(svmr.roc, add = TRUE, col = 7)


auc <- c(glm.roc$auc[1], knn.roc$auc[1], tree.roc$auc[1],
         gbmB.roc$auc[1], rf.roc$auc[1], svml.roc$auc[1], svmr.roc$auc[1])

modelNames <- c("glm","knn","tree","gbmB","rf","svml", "svmr")
legend("bottomright", legend = paste0(modelNames, ": ", round(auc,3)),
       col = 1:7, lwd = 2)
```

### Test model performance in test data

```{r}
pred.svmr <- predict(svmr.fit, newdata = data[-rowTrain,])
pred.svml <- predict(svml.fit, newdata = data[-rowTrain,]) 
matr.svmr = confusionMatrix(data = pred.svmr,
                reference = data$target[-rowTrain])
matr.svml = confusionMatrix(data = pred.svml,
                reference = data$target[-rowTrain])

svmr_error_rate = mean(data[-rowTrain,]$target != pred.svmr, type = "raw")
svml_error_rate = mean(data[-rowTrain,]$target != pred.svml, type = "raw")
svm_test_error = data.frame(test_error_rate = c(svmr_error_rate,svml_error_rate))

matr = cbind(rbind(matr.svmr$overall[1:2], matr.svml$overall[1:2]),
             rbind(matr.svmr$byClass[1:2], matr.svml$byClass[1:2]),
                   svm_test_error
             
                   )
rownames(matr) = c("svmr", "svml")
matr

```


## Comparison of models

```{r}
set.seed(123)
resamp <- resamples(list(knn = knn.fit.c,glm = glmn.fit.c, svml = svml.fit, svmr = svmr.fit, boost = gbmB.fit,randomforest = rf.fit.c, tree = rpart.fit.c)) 
bwplot(resamp)
```

The support vector machine model with linear kernel performs better with higher accuracy and kappa when checking their predictive ability with test data.


```{r}
save.image(file='test_Weijia.RData')
```

